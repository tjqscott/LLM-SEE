@article{10.1145/3715771,
author = {Calikli, G\"{u}l and Alhamed, Mohammed},
title = {Impact of Request Formats on Effort Estimation: Are LLMs Different Than Humans?},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715771},
doi = {10.1145/3715771},
abstract = {Software development Effort Estimation (SEE) comprises predicting the most realistic amount of effort (e.g., in work hours) required to develop or maintain software based on incomplete, uncertain, and noisy input. Expert judgment is the dominant SEE strategy used in the industry. Yet, expert-based judgment can provide inaccurate effort estimates, leading to projects’ poor budget planning and cost and time overruns, negatively impacting the world economy. Large Language Models (LLMs) are good candidates to assist software professionals in effort estimation. However, their effective leveraging for SEE requires thoroughly investigating their limitations and to what extent they overlap with those of (human) software practitioners. One primary limitation of LLMs is the sensitivity of their responses to prompt changes. Similarly, empirical studies showed that changes in the request format (e.g., rephrasing) could impact (human) software professionals’ effort estimates. This paper reports the first study that replicates a series of SEE experiments, which were initially carried out with software professionals (humans) in the literature. Our study aims to investigate how LLMs’ effort estimates change due to the transition from the traditional request format (i.e., "How much effort is required to complete X?”) to the alternative request format (i.e., "How much can be completed in Y work hours?”). Our experiments involved three different LLMs (GPT-4, Gemini 1.5 Pro, Llama 3.1) and 88 software project specifications (per treatment in each experiment), resulting in 880 prompts, in total that we prepared using 704 user stories from three large-scale open-source software projects (Hyperledger Fabric, Mulesoft Mule, Spring XD). Our findings align with the original experiments conducted with software professionals: The first four experiments showed that LLMs provide lower effort estimates due to transitioning from the traditional to the alternative request format. The findings of the fifth and first experiments detected that LLMs display patterns analogous to anchoring bias, a human cognitive bias defined as the tendency to stick to the anchor (i.e., the "Y work-hours” in the alternative request format). Our findings provide crucial insights into facilitating future human-AI collaboration and prompt designs for improved effort estimation accuracy.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE051},
numpages = {22},
keywords = {Cognitive bias, Empirical software engineering, Human judgement, Large Language Models (LLMs), Software effort estimation}
}

@inproceedings{10.1145/3706598.3713319,
author = {Rosbach, Emely and Ammeling, Jonas and Kr\"{u}gel, Sebastian and Kie\ss{}ig, Angelika and Fritz, Alexis and Ganz, Jonathan and Puget, Chlo\'{e} and Donovan, Taryn and Klang, Andrea and K\"{o}ller, Maximilian C. and Bolfa, Pompei and Tecilla, Marco and Denk, Daniela and Kiupel, Matti and Paraschou, Georgios and Kok, Mun Keong and Haake, Alexander F. H. and de Krijger, Ronald R. and Sonnen, Andreas F.-P. and Kasantikul, Tanit and Dorrestein, Gerry M. and Smedley, Rebecca C. and Stathonikos, Nikolas and Uhl, Matthias and Bertram, Christof A. and Riener, Andreas and Aubreville, Marc},
title = {"When Two Wrongs Don't Make a Right" - Examining Confirmation Bias and the Role of Time Pressure During Human-AI Collaboration in Computational Pathology},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713319},
doi = {10.1145/3706598.3713319},
abstract = {Artificial intelligence (AI)-based decision support systems hold promise for enhancing diagnostic accuracy and efficiency in computational pathology. However, human-AI collaboration can introduce and amplify cognitive biases, like confirmation bias caused by false confirmation when erroneous human opinions are reinforced by inaccurate AI output. This bias may increase under time pressure, a ubiquitous factor in routine pathology, as it strains practitioners’ cognitive resources. We quantified confirmation bias triggered by AI-induced false confirmation and examined the role of time constraints in a web-based experiment, where trained pathology experts (n=28) estimated tumor cell percentages. Our results suggest that AI integration fuels confirmation bias, evidenced by a statistically significant positive linear-mixed-effects model coefficient linking AI recommendations mirroring flawed human judgment and alignment with system advice. Conversely, time pressure appeared to weaken this relationship. These findings highlight potential risks of AI in healthcare and aim to support the safe integration of clinical decision support systems.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {528},
numpages = {18},
keywords = {Cognitive Bias, Confirmation Bias, Time Pressure, Artificial Intelligence, Decision Support Systems, Clinical Decision Support Systems, Computational Pathology, Healthcare},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3167132.3167293,
author = {Shepperd, Martin and Mair, Carolyn and J\o{}rgensen, Magne},
title = {An experimental evaluation of a de-biasing intervention for professional software developers},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167293},
doi = {10.1145/3167132.3167293},
abstract = {Context: The role of expert judgement is essential in our quest to improve software project planning and execution. However, its accuracy is dependent on many factors, not least the avoidance of judgement biases, such as the anchoring bias, arising from being influenced by initial information, even when it's misleading or irrelevant. This strong effect is widely documented.Objective: We aimed to replicate this anchoring bias using professionals and, novel in a software engineering context, explore de-biasing interventions through increasing knowledge and awareness of judgement biases.Method: We ran two series of experiments in company settings with a total of 410 software developers. Some developers took part in a workshop to heighten their awareness of a range of cognitive biases, including anchoring. Later, the anchoring bias was induced by presenting low or high productivity values, followed by the participants' estimates of their own project productivity. Our hypothesis was that the workshop would lead to reduced bias, i.e., work as a de-biasing intervention.Results: The anchors had a large effect (robust Cohen's d = 1.19) in influencing estimates. This was substantially reduced in those participants who attended the workshop (robust Cohen's d = 0.72). The reduced bias related mainly to the high anchor. The de-biasing intervention also led to a threefold reduction in estimate variance.Conclusion: The impact of anchors upon judgement was substantial. Learning about judgement biases does appear capable of mitigating, although not removing, the anchoring bias. The positive effect of de-biasing through learning about biases suggests that it has value.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1510–1517},
numpages = {8},
keywords = {cognitive bias, expert judgement, software effort estimation, software engineering experimentation},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/1081706.1081761,
author = {Aranda, Jorge and Easterbrook, Steve},
title = {Anchoring and adjustment in software estimation},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081761},
doi = {10.1145/1081706.1081761},
abstract = {Anchoring and adjustment is a form of cognitive bias that affects judgments under uncertainty. If given an initial answer, the respondent seems to use this as an 'anchor', adjusting it to reach a more plausible answer, even if the anchor is obviously incorrect. The adjustment is frequently insufficient and so the final answer is biased. In this paper, we report a study to investigate the effects of this phenomenon on software estimation processes. The results show that anchoring and adjustment does occur in software estimation, and can significantly change the resulting estimates, no matter what estimation technique is used. The results also suggest that, considering the magnitude of this bias, software estimators tend to be too confident of their own estimations.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {346–355},
numpages = {10},
keywords = {anchoring and adjustment, cognitive bias, effort estimation, empirical software engineering},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@inproceedings{10.1145/3663384.3663404,
author = {Ahmetoglu, Yoana and Brumby, Duncan and Cox, Anna},
title = {Bridging the Gap Between Time Management Research and Task Management App Design: A Study on the Integration of Planning Fallacy Mitigation Strategies},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663404},
doi = {10.1145/3663384.3663404},
abstract = {Accurate time estimations are vital for meeting deadlines and reducing work-related stress, yet individuals frequently succumb to a wide-spread cognitive bias, the planning fallacy, resulting in poor time management. This research article reports on two studies aimed at addressing this challenge. First, through a review of the psychological literature, we identify four key strategies recommended by research for supporting accurate time estimations in daily tasks. These strategies serve as the foundation for the second study, where we conduct a functionality analysis of prevalent personal task management apps to investigate their alignment with the identified strategies. Our analysis reveals a significant disparity: while research-informed strategies are recommended, they are rarely implemented to a good standard in current apps. This discrepancy emphasizes the importance of addressing this gap between theory and practice. By highlighting the need for future efforts to focus on aiding workers in task duration estimation, this study identifies opportunities for improving the design of task management software to enhance user productivity and alleviate stress.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {12},
numpages = {14},
keywords = {Functionality Review, Literature Review, Personal Task Management, Planning, Planning Fallacy, Time Management, To-Do List},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3715275.3732016,
author = {Sargeant, Holli and Waldetoft, Hannes and Magnusson, M\r{a}ns},
title = {Classifying Hate: Legal and Ethical Evaluations of ML-Assisted Hate Crime Classification and Estimation in Sweden},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732016},
doi = {10.1145/3715275.3732016},
abstract = {Hate crimes, driven by biases against specific demographic groups, harm not only individuals but undermine the security, trust, and cohesion of entire communities. Accurately identifying such crimes remains a significant challenge due to under-reporting, limited training, and the complexity of determining bias motivations. In this paper, we analyze the results of a text classification model developed to improve the precision of hate crime statistics and identification in Sweden. Empirical results indicate the model outperforms traditional manual police classification of hate crimes, achieving higher precision across various crime types and regions. We further disaggregate performance to pinpoint persistent challenges and highlight categories where both human and machine decision-makers struggle. While the model focuses on statistical estimation rather than direct case-level decision-making, we discuss the broader implications of algorithmic transparency, accountability, and explainability. Ultimately, this research illustrates how transformer-based neural networks can responsibly bolster the detection and understanding of hate crimes, informing policies to better protect vulnerable communities.Content warning: This article includes direct quotations and descriptions from hate crime reports, containing offensive language, hateful symbols, and references to discriminatory actions.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {195–208},
numpages = {14},
keywords = {Hate Crime Classification, Automated Crime Classification, Algorithmic Bias, Criminal Justice, Responsible AI, Procedural Justice},
location = {
},
series = {FAccT '25}
}

@article{10.1145/3571810,
author = {Roffarello, Alberto Monge and De Russis, Luigi},
title = {Achieving Digital Wellbeing Through Digital Self-control Tools: A Systematic Review and Meta-analysis},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3571810},
doi = {10.1145/3571810},
abstract = {Public media and researchers in different areas have recently focused on perhaps unexpected problems that derive from an excessive and frequent use of technology, giving rise to a new kind of psychological “digital” wellbeing. Such a novel and pressing topic has fostered, both in the academia and in the industry, the emergence of a variety of digital self-control tools allowing users to self-regulate their technology use through interventions like timers and lock-out mechanisms. While these emerging technologies for behavior change hold great promise to support people’s digital wellbeing, we still have a limited understanding of their real effectiveness, as well as of how to best design and evaluate them. Aiming to guide future research in this important domain, this article presents a systematic review and a meta-analysis of current work on tools for digital self-control. We surface motivations, strategies, design choices, and challenges that characterize the design, development, and evaluation of digital self-control tools. Furthermore, we estimate their overall effect size on reducing (unwanted) technology use through a meta-analysis. By discussing our findings, we provide insights on how to (i) overcome a limited perspective that exclusively focuses on technology overuse and self-monitoring tools, (ii) evaluate digital self-control tools through long-term studies and standardized measures, and (iii) bring ethics in the digital wellbeing discourse and deal with the business model of contemporary tech companies.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {53},
numpages = {66},
keywords = {Digital self-control tools, digital wellbeing, digital overuse, behavior change, persuasive technology}
}

@inproceedings{10.1145/3461702.3462571,
author = {Bhatt, Umang and Antor\'{a}n, Javier and Zhang, Yunfeng and Liao, Q. Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melan\c{c}on, Gabrielle and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and Nachman, Lama and Chunara, Rumi and Srikumar, Madhulika and Weller, Adrian and Xiang, Alice},
title = {Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462571},
doi = {10.1145/3461702.3462571},
abstract = {Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {401–413},
numpages = {13},
keywords = {machine learning, transparency, uncertainty, visualization},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/2968220.2968240,
author = {Li, Yi-Na and Li, Dong-Jin},
title = {How People Deploy Dimensional Information to Estimate Relative Values},
year = {2016},
isbn = {9781450341493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968220.2968240},
doi = {10.1145/2968220.2968240},
abstract = {A high quality decision-making requires accurate estimations of relative values. When one estimates relative values relying on visual stimuli, perceptual bias may weaken the accuracy and bring about risks. This research explores how people estimate relative values heuristically using visual cues with different dimensional information, i.e., linear, areal and volumetric information. We conduct experiments to empirically testify the influences of dimensional information on perceptual biases. First, we confirm the conspicuity of areal information. When instructed to estimate rates exclusively relying on either linear or volumetric information, people would inevitably be influenced by the corresponding rates determined by areal information. Second, we provide evidences that visual cues implying depth would lead to overestimates. This research provides implications for designers to enhance the power of visual persuasions, and for users to improve their decision making quality.},
booktitle = {Proceedings of the 9th International Symposium on Visual Information Communication and Interaction},
pages = {83–90},
numpages = {8},
keywords = {Information visualization, dimensional information, perceptual bias, relative value estimation, visual cue},
location = {Dallas, TX, USA},
series = {VINCI '16}
}

