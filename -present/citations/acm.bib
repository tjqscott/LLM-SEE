@article{10.1145/3715771,
author = {Calikli, G\"{u}l and Alhamed, Mohammed},
title = {Impact of Request Formats on Effort Estimation: Are LLMs Different Than Humans?},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {FSE},
url = {https://doi.org/10.1145/3715771},
doi = {10.1145/3715771},
abstract = {Software development Effort Estimation (SEE) comprises predicting the most realistic amount of effort (e.g., in work hours) required to develop or maintain software based on incomplete, uncertain, and noisy input. Expert judgment is the dominant SEE strategy used in the industry. Yet, expert-based judgment can provide inaccurate effort estimates, leading to projects’ poor budget planning and cost and time overruns, negatively impacting the world economy. Large Language Models (LLMs) are good candidates to assist software professionals in effort estimation. However, their effective leveraging for SEE requires thoroughly investigating their limitations and to what extent they overlap with those of (human) software practitioners. One primary limitation of LLMs is the sensitivity of their responses to prompt changes. Similarly, empirical studies showed that changes in the request format (e.g., rephrasing) could impact (human) software professionals’ effort estimates. This paper reports the first study that replicates a series of SEE experiments, which were initially carried out with software professionals (humans) in the literature. Our study aims to investigate how LLMs’ effort estimates change due to the transition from the traditional request format (i.e., "How much effort is required to complete X?”) to the alternative request format (i.e., "How much can be completed in Y work hours?”). Our experiments involved three different LLMs (GPT-4, Gemini 1.5 Pro, Llama 3.1) and 88 software project specifications (per treatment in each experiment), resulting in 880 prompts, in total that we prepared using 704 user stories from three large-scale open-source software projects (Hyperledger Fabric, Mulesoft Mule, Spring XD). Our findings align with the original experiments conducted with software professionals: The first four experiments showed that LLMs provide lower effort estimates due to transitioning from the traditional to the alternative request format. The findings of the fifth and first experiments detected that LLMs display patterns analogous to anchoring bias, a human cognitive bias defined as the tendency to stick to the anchor (i.e., the "Y work-hours” in the alternative request format). Our findings provide crucial insights into facilitating future human-AI collaboration and prompt designs for improved effort estimation accuracy.},
journal = {Proc. ACM Softw. Eng.},
month = jun,
articleno = {FSE051},
numpages = {22},
keywords = {Cognitive bias, Empirical software engineering, Human judgement, Large Language Models (LLMs), Software effort estimation}
}

@inproceedings{10.1145/3706598.3713319,
author = {Rosbach, Emely and Ammeling, Jonas and Kr\"{u}gel, Sebastian and Kie\ss{}ig, Angelika and Fritz, Alexis and Ganz, Jonathan and Puget, Chlo\'{e} and Donovan, Taryn and Klang, Andrea and K\"{o}ller, Maximilian C. and Bolfa, Pompei and Tecilla, Marco and Denk, Daniela and Kiupel, Matti and Paraschou, Georgios and Kok, Mun Keong and Haake, Alexander F. H. and de Krijger, Ronald R. and Sonnen, Andreas F.-P. and Kasantikul, Tanit and Dorrestein, Gerry M. and Smedley, Rebecca C. and Stathonikos, Nikolas and Uhl, Matthias and Bertram, Christof A. and Riener, Andreas and Aubreville, Marc},
title = {"When Two Wrongs Don't Make a Right" - Examining Confirmation Bias and the Role of Time Pressure During Human-AI Collaboration in Computational Pathology},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713319},
doi = {10.1145/3706598.3713319},
abstract = {Artificial intelligence (AI)-based decision support systems hold promise for enhancing diagnostic accuracy and efficiency in computational pathology. However, human-AI collaboration can introduce and amplify cognitive biases, like confirmation bias caused by false confirmation when erroneous human opinions are reinforced by inaccurate AI output. This bias may increase under time pressure, a ubiquitous factor in routine pathology, as it strains practitioners’ cognitive resources. We quantified confirmation bias triggered by AI-induced false confirmation and examined the role of time constraints in a web-based experiment, where trained pathology experts (n=28) estimated tumor cell percentages. Our results suggest that AI integration fuels confirmation bias, evidenced by a statistically significant positive linear-mixed-effects model coefficient linking AI recommendations mirroring flawed human judgment and alignment with system advice. Conversely, time pressure appeared to weaken this relationship. These findings highlight potential risks of AI in healthcare and aim to support the safe integration of clinical decision support systems.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {528},
numpages = {18},
keywords = {Cognitive Bias, Confirmation Bias, Time Pressure, Artificial Intelligence, Decision Support Systems, Clinical Decision Support Systems, Computational Pathology, Healthcare},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3167132.3167293,
author = {Shepperd, Martin and Mair, Carolyn and J\o{}rgensen, Magne},
title = {An experimental evaluation of a de-biasing intervention for professional software developers},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167293},
doi = {10.1145/3167132.3167293},
abstract = {Context: The role of expert judgement is essential in our quest to improve software project planning and execution. However, its accuracy is dependent on many factors, not least the avoidance of judgement biases, such as the anchoring bias, arising from being influenced by initial information, even when it's misleading or irrelevant. This strong effect is widely documented.Objective: We aimed to replicate this anchoring bias using professionals and, novel in a software engineering context, explore de-biasing interventions through increasing knowledge and awareness of judgement biases.Method: We ran two series of experiments in company settings with a total of 410 software developers. Some developers took part in a workshop to heighten their awareness of a range of cognitive biases, including anchoring. Later, the anchoring bias was induced by presenting low or high productivity values, followed by the participants' estimates of their own project productivity. Our hypothesis was that the workshop would lead to reduced bias, i.e., work as a de-biasing intervention.Results: The anchors had a large effect (robust Cohen's d = 1.19) in influencing estimates. This was substantially reduced in those participants who attended the workshop (robust Cohen's d = 0.72). The reduced bias related mainly to the high anchor. The de-biasing intervention also led to a threefold reduction in estimate variance.Conclusion: The impact of anchors upon judgement was substantial. Learning about judgement biases does appear capable of mitigating, although not removing, the anchoring bias. The positive effect of de-biasing through learning about biases suggests that it has value.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1510–1517},
numpages = {8},
keywords = {cognitive bias, expert judgement, software effort estimation, software engineering experimentation},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/3785367,
author = {Parthasarathy, P D and Joshi, Swaroop},
title = {Accessibility Education for Software Engineers: Evaluating the Impact of Game-Based Learning},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3785367},
doi = {10.1145/3785367},
abstract = {Background: Digital inaccessibility remains a significant barrier to inclusion. WebAIM’s 2025 report indicates that only 5.2\% of the top one million website homepages fully conform to accessibility standards, reflecting a marginal improvement of 3.1\% in Web Content Accessibility Guidelines (WCAG) compliance over the past six years. This stagnation underscores a persistent skills gap within the technology sector, primarily attributed to software engineers’ limited foundational knowledge and technical expertise in digital accessibility. While academic institutions have begun integrating accessibility into computing curricula, effective training strategies for practicing software engineers remain underexplored.Methods: We developed two serious games, A11yMythBuster and A11yBugHunter, designed to enhance accessibility awareness and technical proficiency. The design and evaluation of these games use the Design-Based Research (DBR) approach, with this study representing the first iteration of the iterative development cycle. The impact of these games was assessed through a mixed-methods study involving 125 software engineers. Data collection methods included pre-post surveys, gameplay data, and semi-structured interviews to capture changes in both skills and attitudes.Results: Participants demonstrated substantial improvements in technical accessibility skills following gameplay interventions. The study also revealed a marked attitudinal shift, with engineers showing increased commitment to prioritizing accessibility in their software development practices. Both quantitative metrics and qualitative feedback indicated that the game-based approach effectively bridged knowledge gaps while fostering cultural change within development teams.Conclusions: Game-based learning can effectively address both technical and attitudinal barriers to digital accessibility implementation among software engineers. Our findings suggest that serious games offer a scalable strategy to advance accessibility education in the tech industry. This study contributes to the growing literature on innovative, evidence-based interventions for digital accessibility education and provides a practical approach to improving WCAG compliance in software development.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = dec,
keywords = {digital accessibility, game-based learning (GBL), serious games, Web content accessibility guidelines (WCAG), Teaching-learning of Software professionals}
}

@inproceedings{10.1145/1081706.1081761,
author = {Aranda, Jorge and Easterbrook, Steve},
title = {Anchoring and adjustment in software estimation},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081761},
doi = {10.1145/1081706.1081761},
abstract = {Anchoring and adjustment is a form of cognitive bias that affects judgments under uncertainty. If given an initial answer, the respondent seems to use this as an 'anchor', adjusting it to reach a more plausible answer, even if the anchor is obviously incorrect. The adjustment is frequently insufficient and so the final answer is biased. In this paper, we report a study to investigate the effects of this phenomenon on software estimation processes. The results show that anchoring and adjustment does occur in software estimation, and can significantly change the resulting estimates, no matter what estimation technique is used. The results also suggest that, considering the magnitude of this bias, software estimators tend to be too confident of their own estimations.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {346–355},
numpages = {10},
keywords = {anchoring and adjustment, cognitive bias, effort estimation, empirical software engineering},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@article{10.1145/1095430.1081761,
author = {Aranda, Jorge and Easterbrook, Steve},
title = {Anchoring and adjustment in software estimation},
year = {2005},
issue_date = {September 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1095430.1081761},
doi = {10.1145/1095430.1081761},
abstract = {Anchoring and adjustment is a form of cognitive bias that affects judgments under uncertainty. If given an initial answer, the respondent seems to use this as an 'anchor', adjusting it to reach a more plausible answer, even if the anchor is obviously incorrect. The adjustment is frequently insufficient and so the final answer is biased. In this paper, we report a study to investigate the effects of this phenomenon on software estimation processes. The results show that anchoring and adjustment does occur in software estimation, and can significantly change the resulting estimates, no matter what estimation technique is used. The results also suggest that, considering the magnitude of this bias, software estimators tend to be too confident of their own estimations.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {346–355},
numpages = {10},
keywords = {anchoring and adjustment, cognitive bias, effort estimation, empirical software engineering}
}

@inproceedings{10.1145/3663384.3663404,
author = {Ahmetoglu, Yoana and Brumby, Duncan and Cox, Anna},
title = {Bridging the Gap Between Time Management Research and Task Management App Design: A Study on the Integration of Planning Fallacy Mitigation Strategies},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663404},
doi = {10.1145/3663384.3663404},
abstract = {Accurate time estimations are vital for meeting deadlines and reducing work-related stress, yet individuals frequently succumb to a wide-spread cognitive bias, the planning fallacy, resulting in poor time management. This research article reports on two studies aimed at addressing this challenge. First, through a review of the psychological literature, we identify four key strategies recommended by research for supporting accurate time estimations in daily tasks. These strategies serve as the foundation for the second study, where we conduct a functionality analysis of prevalent personal task management apps to investigate their alignment with the identified strategies. Our analysis reveals a significant disparity: while research-informed strategies are recommended, they are rarely implemented to a good standard in current apps. This discrepancy emphasizes the importance of addressing this gap between theory and practice. By highlighting the need for future efforts to focus on aiding workers in task duration estimation, this study identifies opportunities for improving the design of task management software to enhance user productivity and alleviate stress.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {12},
numpages = {14},
keywords = {Functionality Review, Literature Review, Personal Task Management, Planning, Planning Fallacy, Time Management, To-Do List},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3665689.3665744,
author = {Li, Huidong and Zhou, Ting and Huang, Zheng and Tang, Shuo},
title = {Digital Interventions for Treating Adolescent Anxiety: A Meta-Analysis on Randomized Controlled Trials},
year = {2024},
isbn = {9798400716645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665689.3665744},
doi = {10.1145/3665689.3665744},
abstract = {Purpose: This meta-analysis aimed to explore the effects of digital interventions treating adolescent anxiety and the influencing factors associated with treatment outcomes.Methods: Two researchers conducted a systematic search of randomized controlled studies of digital interventions for treating adolescent anxiety in Pubmed, APA PsycInfo, Science Direct, Web of Science, Scopus, The Cochrane Library, CNKI, WanFang Data and CQVIP. They screened the articles, extracted the data and reviewed the quality of the articles based on Cochrane 5.1.0.. Meta-analyses were conducted using software Review Manager Version 5.4.Results: This study included 18 randomized controlled trials. In contrast to control groups, digital interventions produced medium post-treatment pooled effect sizes regarding anxiety symptoms (g=0.67, 95\% CI 0.40-0.95) and small post-intervention pooled effect sizes regarding depression symptoms (g=0.40, 95\% CI 0.19-0.61). Sub-group analyses showed that therapists’ guidance, therapy basis, number of sessions, and intervention attrition rates significantly influenced effect sizes.Conclusions: Digital interventions are effective in treating adolescent anxiety and depression. Treatment outcomes were influenced by therapists’ guidance, therapy basis, number of sessions, and intervention attrition rates. Future reviews could focus on research on long-term effectiveness and moderating factors for treatment outcomes.},
booktitle = {Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing},
pages = {323–330},
numpages = {8},
location = {Beijing, China},
series = {BIC '24}
}

@inproceedings{10.1145/3576840.3578332,
author = {Liu, Jiqun},
title = {Toward A Two-Sided Fairness Framework in Search and Recommendation},
year = {2023},
isbn = {9798400700354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576840.3578332},
doi = {10.1145/3576840.3578332},
abstract = {As artificial intelligence (AI) assisted search and recommender systems have become ubiquitous in workplaces and everyday lives, understanding and accounting for fairness has gained increasing attention in the design and evaluation of such systems. While there is a growing body of computing research on measuring system fairness and biases associated with data and algorithms, the impact of human biases that go beyond traditional machine learning (ML) pipelines still remain understudied. In this Perspective Paper, we seek to develop a two-sided fairness framework that not only characterizes data and algorithmic biases, but also highlights the cognitive and perceptual biases that may exacerbate system biases and lead to unfair decisions. Within the framework, we also analyze the interactions between human and system biases in search and recommendation episodes. Built upon the two-sided framework, our research synthesizes intervention and intelligent nudging strategies applied in cognitive and algorithmic debiasing, and also proposes novel goals and measures for evaluating the performance of systems in addressing and proactively mitigating the risks associated with biases in data, algorithms, and bounded rationality. This paper uniquely integrates the insights regarding human biases and system biases into a cohesive framework and extends the concept of fairness from human-centered perspective. The extended fairness framework better reflects the challenges and opportunities in users’ interactions with search and recommender systems of varying modalities. Adopting the two-sided approach in information system design has the potential to enhancing both the effectiveness in online debiasing and the usefulness to boundedly rational users engaging in information-intensive decision-making.},
booktitle = {Proceedings of the 2023 Conference on Human Information Interaction and Retrieval},
pages = {236–246},
numpages = {11},
keywords = {Two-sided fairness, human bias, information retrieval, recommender system, system bias},
location = {Austin, TX, USA},
series = {CHIIR '23}
}

@inproceedings{10.1145/3540250.3560877,
author = {Ralph, Paul and Baltes, Sebastian},
title = {Paving the way for mature secondary research: the seven types of literature review},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3560877},
doi = {10.1145/3540250.3560877},
abstract = {Confusion over different kinds of secondary research, and their divergent purposes, is undermining the effectiveness and usefulness of secondary studies in software engineering. This short paper therefore explains the differences between ad hoc review, case survey, critical review, meta-analysis (aka systematic literature review), meta-synthesis (aka thematic analysis), rapid review and scoping review (aka systematic mapping study). These definitions and associated guidelines help researchers better select and describe their literature reviews, while helping reviewers select more appropriate evaluation criteria.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1632–1636},
numpages = {5},
keywords = {Literature review, secondary research, systematic review},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3745676.3745679,
author = {Yang, Bai and Qi, Mingzhen and Huang, Shi},
title = {Comparative Analysis of Input-Output Indicator System Method and DEA Method for Measuring R&amp;D Project Input and Output: An Empirical Study Based on Telecommunication Operators},
year = {2025},
isbn = {9798400715150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745676.3745679},
doi = {10.1145/3745676.3745679},
abstract = {Measuring R&amp;D efficiency is crucial for enterprises to maintain innovation capabilities and optimize resource allocation in an increasingly competitive market. Effective analysis of R&amp;D input-output not only helps companies identify the effectiveness of resource utilization but also provides a scientific basis for strategic decision-making, thereby promoting technological advancement and economic growth. This paper aims to explore the efficiency evaluation of R&amp;D projects by innovatively combining the input-output indicator system method and Data Envelopment Analysis (DEA) method to balance subjective bias and objective analysis while addressing strategic needs.Firstly, through literature review and analysis of leading practices, we constructed an evaluation indicator system for R&amp;D project input-output, which includes five primary dimensions: financial input, personnel input, strategic output, scientific output, and economic output, further detailed into 15 tertiary dimensions. Subsequently, based on the different strategic orientations of the two types of projects, we employed expert scoring to assign weights to each dimension to reflect strategic guidance. Finally, we implemented the DEA method using R language to analyze the efficiency of strategic R&amp;D input projects and compared the results with those obtained from the input-output system method. The research findings indicate that the efficiency measured by the input-output indicator system method is generally lower than that obtained by the DEA method; under both methods, the average efficiency of application-based projects is lower than that of key technology projects. The influence of strategic factors on projects with low efficiency is not significant; for projects with substantial ranking differences under both methods, further research on outputs is needed to clarify resource allocation.},
booktitle = {Proceedings of the 2025 2nd International Conference on Innovation Management and Information System},
pages = {15–24},
numpages = {10},
keywords = {DEA, Indicator System, Input-Output, OAT, R&amp;D Management Efficiency, Strategic R&amp;D},
location = {
},
series = {ICIIS '25}
}

@inproceedings{10.1145/3770177.3770255,
author = {Wang, Pei and Zhong, Qianghui},
title = {Risk Assessment of Target Procurement Price Based on AHP-CRITIC Combined Weighting and K-Means Algorithm},
year = {2025},
isbn = {9798400720109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3770177.3770255},
doi = {10.1145/3770177.3770255},
abstract = {Procurement risk assessment is a relevant aspect to deal with procurement cost, time, and a successful project outcome in modern complex procurement contexts, like defence systems, industrial, and others. The existing risk analysis methodologies tend to use too much on personal expertise that may pose the use of these tools inadequate for dealing with the complexity of the various, mutable, and evolving procurement risk characteristics. To face such a situation, this paper proposes a hybrid risk analysis model that integrates a classical fuzzy comprehensive evaluation model with some features of a machine learning tool, such as the K-Means clustering algorithm. The general purpose of this paper is to get risk estimates, objective and user-friendly, for a procurement process; it is carried out through a hybrid approach, with the combination of an expert-based weighting (based on an analysis-by-objects Weighting in Determining Objectives with AHP and Critic methods), and a machine-learning clustering analysis of the risks factors. It considers a process in which, firstly, the hybrid weights are generated applying hybrid Weighting in Determining Objectives with AHP-CRITIC method and then the K-Means clustering is used for the risk elements clustering based on the weight and the corresponding fuzzy risk degree. Being a clustering and a unsupervised learning algorithm, K-Means allows us to discover the latent patterns and then perform stratification into high level categories, as the potential risk level of a given item, without previous knowledge labeling, a requirement providing a rigorous foundation for a consistent and efficient risk classification and importance prioritization. The results, derived from a case study on naval equipment acquisition, demonstrate that the integration of K-Means significantly improves risk segmentation granularity. Specifically, the algorithm identifies three distinct clusters of risk factors corresponding to varying degrees of perceived severity and strategic relevance. This research contributes to the field by introducing a scalable, transparent, and analytically robust framework for procurement risk management. The fusion of fuzzy evaluation and machine learning supports more informed, adaptive decision-making across the procurement lifecycle, and offers practical value for government, military, and large-scale industrial procurement stakeholders.},
booktitle = {Proceedings of the 2025 International Conference on Economic Management and Big Data Application},
pages = {483–490},
numpages = {8},
keywords = {AHP-CRITIC weighting, Fuzzy comprehensive evaluation, K-Means algorithm, Machine learning, Procurement risk assessment},
location = {
},
series = {ICEMBDA '25}
}

@inproceedings{10.1145/3715669.3727348,
author = {Carminati, Marco and Melloni, Filippo and Marano, Giulio and Pettenella, Alberto and Bani, Daniele and Crafa, Daniele Maria and Aspesi, Andrea and Duchowski, Andrew and Ongarello, Tommaso and Merigo, Luca},
title = {Energy-Aware Benchmarking of Wearable Eye Trackers},
year = {2025},
isbn = {9798400714870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715669.3727348},
doi = {10.1145/3715669.3727348},
abstract = {The number of devices embedding eye tracking (ET) capabilities, such as portable webcam-based consumer devices and wearable ones, such as headsets and smart eyeglasses, is rapidly increasing, making this technology truly pervasive. Despite the large number of papers and reviews discussing data quality and benchmarking of trackers, none of them is addressing the trade-off between power consumption, speed and accuracy. Power dissipation is typically dominated by signal processing to extract gaze information from sensors embedded in the glasses. This compromise is crucial for smart glasses, powered by miniature batteries, offering a typical power budget of a few tens of mW for ET. Here we propose a simple benchmarking flow for wearable trackers, focused on power consumption, as well as accuracy, precision and sampling rate, and based on three complementary test setups. We report the preliminary results of the experimental characterization of 6 commercial trackers in the first static setup and we show a comparison of their performance based on a single figure of merit.},
booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications},
articleno = {118},
numpages = {7},
keywords = {eye tracking, validation, data quality, standard},
location = {
},
series = {ETRA '25}
}

@inproceedings{10.1145/3715275.3732016,
author = {Sargeant, Holli and Waldetoft, Hannes and Magnusson, M\r{a}ns},
title = {Classifying Hate: Legal and Ethical Evaluations of ML-Assisted Hate Crime Classification and Estimation in Sweden},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732016},
doi = {10.1145/3715275.3732016},
abstract = {Hate crimes, driven by biases against specific demographic groups, harm not only individuals but undermine the security, trust, and cohesion of entire communities. Accurately identifying such crimes remains a significant challenge due to under-reporting, limited training, and the complexity of determining bias motivations. In this paper, we analyze the results of a text classification model developed to improve the precision of hate crime statistics and identification in Sweden. Empirical results indicate the model outperforms traditional manual police classification of hate crimes, achieving higher precision across various crime types and regions. We further disaggregate performance to pinpoint persistent challenges and highlight categories where both human and machine decision-makers struggle. While the model focuses on statistical estimation rather than direct case-level decision-making, we discuss the broader implications of algorithmic transparency, accountability, and explainability. Ultimately, this research illustrates how transformer-based neural networks can responsibly bolster the detection and understanding of hate crimes, informing policies to better protect vulnerable communities.Content warning: This article includes direct quotations and descriptions from hate crime reports, containing offensive language, hateful symbols, and references to discriminatory actions.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {195–208},
numpages = {14},
keywords = {Hate Crime Classification, Automated Crime Classification, Algorithmic Bias, Criminal Justice, Responsible AI, Procedural Justice},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.1145/3613904.3642738,
author = {Haque, MD Romael and Saxena, Devansh and Weathington, Katy and Chudzik, Joseph and Guha, Shion},
title = {Are We Asking the Right Questions?: Designing for Community Stakeholders’ Interactions with AI in Policing},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642738},
doi = {10.1145/3613904.3642738},
abstract = {Research into recidivism risk prediction in the criminal justice system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making. This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context. We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making. Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them. Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps. Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {301},
numpages = {20},
keywords = {algorithmic crime mapping, human-AI decision-making, problem formulation, public sector algorithms},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inbook{10.1145/3715014.3722045,
author = {Duan, Di and Lyu, Shengzhe and Yuan, Mu and Xue, Hongfei and Li, Tianxing and Xu, Weitao and Wu, Kaishun and Xing, Guoliang},
title = {Argus: Multi-View Egocentric Human Mesh Reconstruction Based on Stripped-Down Wearable mmWave Add-on},
year = {2025},
isbn = {9798400714795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715014.3722045},
abstract = {In this paper, we propose Argus, a wearable add-on system based on stripped-down (i.e., compact, lightweight, low-power, limited-capability) mmWave radars. It is the first to achieve egocentric human mesh reconstruction in a multi-view manner. Compared with conventional frontal-view mmWave sensing solutions, it addresses several pain points, such as restricted sensing range, occlusion, and the multipath effect caused by surroundings. To overcome the limited capabilities of the stripped-down mmWave radars (with only one transmit antenna and three receive antennas), we tackle three main challenges and propose a holistic solution, including tailored hardware design, sophisticated signal processing, and a deep neural network optimized for high-dimensional complex point clouds. Extensive evaluation shows that Argus achieves performance comparable to traditional solutions based on high-capability mmWave radars, with an average vertex error of 6.5 cm, solely using stripped-down radars deployed in a multi-view configuration. It presents robustness and practicality across conditions, such as with unseen users and different host devices.},
booktitle = {Proceedings of the 23rd ACM Conference on Embedded Networked Sensor Systems},
pages = {1–14},
numpages = {14}
}

@inproceedings{10.1145/3558819.3565206,
author = {Sun, Ludi and Li, Xing and Li, Changtai},
title = {The Recognition of Different Angles of Pattern in Mice through Computer-Based Visual Discrimination},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3565206},
doi = {10.1145/3558819.3565206},
abstract = {A primary component of visual recognition is perception of the local angle of objects or scenes. Knowing the capacity of different angles discrimination in mice can be constructive. For that, we conducted an experiment based on the Water Visual Task. And mice were trained to escape water and swim to the platform by distinguishing the different angles patterns display in the screen. In our way, the recognition threshold for mice is in the range of 5°to 15°.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {855–859},
numpages = {5},
location = {Brisbane, QLD, Australia},
series = {ICCSIE '22}
}

@article{10.1145/3643552,
author = {Wang, Chongyang and Feng, Yuan and Zhong, Lingxiao and Zhu, Siyi and Zhang, Chi and Zheng, Siqi and Liang, Chen and Wang, Yuntao and He, Chengqi and Yu, Chun and Shi, Yuanchun},
title = {UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1},
url = {https://doi.org/10.1145/3643552},
doi = {10.1145/3643552},
abstract = {We introduce UbiPhysio, a milestone framework that delivers fine-grained action description and feedback in natural language to support people's daily functioning, fitness, and rehabilitation activities. This expert-like capability assists users in properly executing actions and maintaining engagement in remote fitness and rehabilitation programs. Specifically, the proposed UbiPhysio framework comprises a fine-grained action descriptor and a knowledge retrieval-enhanced feedback module. The action descriptor translates action data, represented by a set of biomechanical movement features we designed based on clinical priors, into textual descriptions of action types and potential movement patterns. Building on physiotherapeutic domain knowledge, the feedback module provides clear and engaging expert feedback. We evaluated UbiPhysio's performance through extensive experiments with data from 104 diverse participants, collected in a home-like setting during 25 types of everyday activities and exercises. We assessed the quality of the language output under different tuning strategies using standard benchmarks. We conducted a user study to gather insights from clinical physiotherapists and potential users about our framework. Our initial tests show promise for deploying UbiPhysio in real-life settings without specialized devices.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {20},
numpages = {27},
keywords = {action understanding, activities of daily life, feedback generation, fitness, rehabilitation}
}

@article{10.1145/3571810,
author = {Roffarello, Alberto Monge and De Russis, Luigi},
title = {Achieving Digital Wellbeing Through Digital Self-control Tools: A Systematic Review and Meta-analysis},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3571810},
doi = {10.1145/3571810},
abstract = {Public media and researchers in different areas have recently focused on perhaps unexpected problems that derive from an excessive and frequent use of technology, giving rise to a new kind of psychological “digital” wellbeing. Such a novel and pressing topic has fostered, both in the academia and in the industry, the emergence of a variety of digital self-control tools allowing users to self-regulate their technology use through interventions like timers and lock-out mechanisms. While these emerging technologies for behavior change hold great promise to support people’s digital wellbeing, we still have a limited understanding of their real effectiveness, as well as of how to best design and evaluate them. Aiming to guide future research in this important domain, this article presents a systematic review and a meta-analysis of current work on tools for digital self-control. We surface motivations, strategies, design choices, and challenges that characterize the design, development, and evaluation of digital self-control tools. Furthermore, we estimate their overall effect size on reducing (unwanted) technology use through a meta-analysis. By discussing our findings, we provide insights on how to (i) overcome a limited perspective that exclusively focuses on technology overuse and self-monitoring tools, (ii) evaluate digital self-control tools through long-term studies and standardized measures, and (iii) bring ethics in the digital wellbeing discourse and deal with the business model of contemporary tech companies.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {53},
numpages = {66},
keywords = {Digital self-control tools, digital wellbeing, digital overuse, behavior change, persuasive technology}
}

@inproceedings{10.1145/3544548.3581058,
author = {Ma, Shuai and Lei, Ying and Wang, Xinru and Zheng, Chengbo and Shi, Chuhan and Yin, Ming and Ma, Xiaojuan},
title = {Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581058},
doi = {10.1145/3544548.3581058},
abstract = {In AI-assisted decision-making, it is critical for human decision-makers to know when to trust AI and when to trust themselves. However, prior studies calibrated human trust only based on AI confidence indicating AI’s correctness likelihood (CL) but ignored humans’ CL, hindering optimal team decision-making. To mitigate this gap, we proposed to promote humans’ appropriate trust based on the CL of both sides at a task-instance level. We first modeled humans’ CL by approximating their decision-making models and computing their potential performance in similar instances. We demonstrated the feasibility and effectiveness of our model via two preliminary studies. Then, we proposed three CL exploitation strategies to calibrate users’ trust explicitly/implicitly in the AI-assisted decision-making process. Results from a between-subjects experiment (N=293) showed that our CL exploitation strategies promoted more appropriate human trust in AI, compared with only using AI confidence. We further provided practical implications for more human-compatible AI-assisted decision-making.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {759},
numpages = {19},
keywords = {AI-Assisted Decision-making, Human-AI Collaboration, Trust Calibration, Trust in AI},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3565570,
author = {Nafees, Muhammad Nouman and Saxena, Neetesh and Cardenas, Alvaro and Grijalva, Santiago and Burnap, Pete},
title = {Smart Grid Cyber-Physical Situational Awareness of Complex Operational Technology Attacks: A Review},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3565570},
doi = {10.1145/3565570},
abstract = {The smart grid (SG), regarded as the complex cyber-physical ecosystem of infrastructures, orchestrates advanced communication, computation, and control technologies to interact with the physical environment. Due to the high rewards that threats to the grid can realize, adversaries can mount complex cyber-attacks such as advanced persistent threats-based and coordinated attacks to cause operational malfunctions and power outages in the worst scenarios: The latter of which was reflected in the Ukrainian power grid attack. Despite widespread research on smart grid security, the impact of targeted attacks on control and power systems is anecdotal. This article reviews the smart grid security from collaborative factors, emphasizing the situational awareness (SA). Specifically, we propose a threat modeling framework and review the nature of cyber-physical attacks to understand their characteristics and impacts on the smart grid’s control and physical systems. We examine the existing threats detection and defense capabilities, such as intrusion detection systems (IDSs), moving target defense (MTD), and co-simulation techniques, along with discussing the impact of attacks through situational awareness and power system metrics. We discuss the human factor aspects for power system operators in analyzing the impacts of cyber-attacks. Finally, we investigate the research challenges with key research gaps to shed light on future research directions.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {215},
numpages = {36},
keywords = {Smart grid, threat modeling, complex cyber-physical attacks, operational technology attacks, intrusion detection system, deep learning, federated learning, moving target defense, co-simulation, situational awareness, metrics}
}

@inproceedings{10.1145/3461702.3462571,
author = {Bhatt, Umang and Antor\'{a}n, Javier and Zhang, Yunfeng and Liao, Q. Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melan\c{c}on, Gabrielle and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and Nachman, Lama and Chunara, Rumi and Srikumar, Madhulika and Weller, Adrian and Xiang, Alice},
title = {Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462571},
doi = {10.1145/3461702.3462571},
abstract = {Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {401–413},
numpages = {13},
keywords = {machine learning, transparency, uncertainty, visualization},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/2968220.2968240,
author = {Li, Yi-Na and Li, Dong-Jin},
title = {How People Deploy Dimensional Information to Estimate Relative Values},
year = {2016},
isbn = {9781450341493},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968220.2968240},
doi = {10.1145/2968220.2968240},
abstract = {A high quality decision-making requires accurate estimations of relative values. When one estimates relative values relying on visual stimuli, perceptual bias may weaken the accuracy and bring about risks. This research explores how people estimate relative values heuristically using visual cues with different dimensional information, i.e., linear, areal and volumetric information. We conduct experiments to empirically testify the influences of dimensional information on perceptual biases. First, we confirm the conspicuity of areal information. When instructed to estimate rates exclusively relying on either linear or volumetric information, people would inevitably be influenced by the corresponding rates determined by areal information. Second, we provide evidences that visual cues implying depth would lead to overestimates. This research provides implications for designers to enhance the power of visual persuasions, and for users to improve their decision making quality.},
booktitle = {Proceedings of the 9th International Symposium on Visual Information Communication and Interaction},
pages = {83–90},
numpages = {8},
keywords = {Information visualization, dimensional information, perceptual bias, relative value estimation, visual cue},
location = {Dallas, TX, USA},
series = {VINCI '16}
}

